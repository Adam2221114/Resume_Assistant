from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸ”„ æ­£åœ¨åŠ è½½ Qwen-1.8B-Chat æ¨¡å‹ï¼Œè¯·ç¨ç­‰...")

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-1_8B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-1_8B-Chat",
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
).eval()

print("ğŸ¤– Qwen é¢è¯•æœºå™¨äººå·²å‡†å¤‡å°±ç»ªï¼è¾“å…¥ 'exit' å¯é€€å‡ºå¯¹è¯")

while True:
    user_input = input("\nğŸ‘¤ ä½ ï¼š")
    if user_input.lower().strip() in ["exit", "quit"]:
        print("âœ… é€€å‡ºæˆåŠŸï¼Œå†è§ï¼")
        break

    inputs = tokenizer(user_input, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.8)
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    print("ğŸ¤– AIï¼š", response)
